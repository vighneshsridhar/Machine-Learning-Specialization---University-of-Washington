{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"name":"CLA03-NB01.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2v-56JsVnrcG"},"source":["# Implementing logistic regression from scratch\n","\n","The goal of this notebook is to implement your own logistic regression classifier. You will:\n","\n"," * Extract features from Amazon product reviews.\n"," * Convert an SFrame into a NumPy array.\n"," * Implement the link function for logistic regression.\n"," * Write a function to compute the derivative of the log likelihood function with respect to a single coefficient.\n"," * Implement gradient ascent.\n"," * Given a set of coefficients, predict sentiments.\n"," * Compute classification accuracy for the logistic regression model.\n"," \n","Let's get started!\n","    \n","## Fire up Turi Create\n","\n","Make sure you have the latest version of Turi Create."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"S_kFkK69nrcJ","executionInfo":{"status":"ok","timestamp":1630098704879,"user_tz":420,"elapsed":191355,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"eaa8216c-5ee5-4850-d9d3-859cae33edbb"},"source":["from __future__ import division\n","!pip install turicreate\n","import turicreate\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import numpy as np\n","import math\n","import string"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting turicreate\n","  Downloading turicreate-6.4.1-cp37-cp37m-manylinux1_x86_64.whl (92.0 MB)\n","\u001b[K     |████████████████████████████████| 92.0 MB 14 kB/s \n","\u001b[?25hRequirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (7.1.2)\n","Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from turicreate) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.15.0)\n","Collecting prettytable==0.7.2\n","  Downloading prettytable-0.7.2.zip (28 kB)\n","Collecting tensorflow<2.1.0,>=2.0.0\n","  Downloading tensorflow-2.0.4-cp37-cp37m-manylinux2010_x86_64.whl (86.4 MB)\n","\u001b[K     |████████████████████████████████| 86.4 MB 18 kB/s \n","\u001b[?25hCollecting coremltools==3.3\n","  Downloading coremltools-3.3-cp37-none-manylinux1_x86_64.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 42.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.4.1)\n","Requirement already satisfied: pandas>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.1.5)\n","Requirement already satisfied: decorator>=4.0.9 in /usr/local/lib/python3.7/dist-packages (from turicreate) (4.4.2)\n","Collecting resampy==0.2.1\n","  Downloading resampy-0.2.1.tar.gz (322 kB)\n","\u001b[K     |████████████████████████████████| 322 kB 64.2 MB/s \n","\u001b[?25hCollecting numba<0.51.0\n","  Downloading numba-0.50.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[K     |████████████████████████████████| 3.6 MB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from turicreate) (1.19.5)\n","Requirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from coremltools==3.3->turicreate) (3.17.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<0.51.0->turicreate) (57.4.0)\n","Collecting llvmlite<0.34,>=0.33.0.dev0\n","  Downloading llvmlite-0.33.0-cp37-cp37m-manylinux1_x86_64.whl (18.3 MB)\n","\u001b[K     |████████████████████████████████| 18.3 MB 69 kB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->turicreate) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.2->turicreate) (2.8.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->turicreate) (2.10)\n","Collecting h5py<=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 45.4 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.39.0)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 27.1 MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.8.1)\n","Collecting numpy\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.12.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.1.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (1.12.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.1.0,>=2.0.0->turicreate) (0.37.0)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n","\u001b[K     |████████████████████████████████| 449 kB 70.3 MB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.4.5)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.34.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.2.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.7.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (4.6.4)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow<2.1.0,>=2.0.0->turicreate) (3.7.4.3)\n","Building wheels for collected packages: prettytable, resampy, gast\n","  Building wheel for prettytable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for prettytable: filename=prettytable-0.7.2-py3-none-any.whl size=13714 sha256=f21bdc31c006128e8f20ffcd9ecbb456a8295fc321032918139a7c4262093b5e\n","  Stored in directory: /root/.cache/pip/wheels/b2/7f/f6/f180315b584f00445045ff1699b550fa895d09471337ce21c6\n","  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for resampy: filename=resampy-0.2.1-py3-none-any.whl size=320860 sha256=3a3366598751330ec7c5ccf0f0b28d11fb621fc4eb9e3f98d02f2386a3b99e01\n","  Stored in directory: /root/.cache/pip/wheels/71/74/53/d5ceb7c5ee7a168c7d106041863e71ac3273f4a4677743a284\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=74780fd3223081434dc8e07fac3b459dcd3e22947dd7f476e5779471b3994c85\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built prettytable resampy gast\n","Installing collected packages: numpy, llvmlite, h5py, tensorflow-estimator, tensorboard, numba, keras-applications, gast, tensorflow, resampy, prettytable, coremltools, turicreate\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: llvmlite\n","    Found existing installation: llvmlite 0.34.0\n","    Uninstalling llvmlite-0.34.0:\n","      Successfully uninstalled llvmlite-0.34.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.6.0\n","    Uninstalling tensorflow-estimator-2.6.0:\n","      Successfully uninstalled tensorflow-estimator-2.6.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.6.0\n","    Uninstalling tensorboard-2.6.0:\n","      Successfully uninstalled tensorboard-2.6.0\n","  Attempting uninstall: numba\n","    Found existing installation: numba 0.51.2\n","    Uninstalling numba-0.51.2:\n","      Successfully uninstalled numba-0.51.2\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.6.0\n","    Uninstalling tensorflow-2.6.0:\n","      Successfully uninstalled tensorflow-2.6.0\n","  Attempting uninstall: resampy\n","    Found existing installation: resampy 0.2.2\n","    Uninstalling resampy-0.2.2:\n","      Successfully uninstalled resampy-0.2.2\n","  Attempting uninstall: prettytable\n","    Found existing installation: prettytable 2.1.0\n","    Uninstalling prettytable-2.1.0:\n","      Successfully uninstalled prettytable-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n","librosa 0.8.1 requires resampy>=0.2.2, but you have resampy 0.2.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed coremltools-3.3 gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 llvmlite-0.33.0 numba-0.50.1 numpy-1.18.5 prettytable-0.7.2 resampy-0.2.1 tensorboard-2.0.2 tensorflow-2.0.4 tensorflow-estimator-2.0.1 turicreate-6.4.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}},{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-1yA_h-ZnrcK"},"source":["## Load review dataset"]},{"cell_type":"markdown","metadata":{"id":"DyhrZ7IGnrcL"},"source":["For this assignment, we will use a subset of the Amazon product review dataset. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews."]},{"cell_type":"code","metadata":{"id":"G38qzKCInrcM","executionInfo":{"status":"ok","timestamp":1630101350610,"user_tz":420,"elapsed":166,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["products = turicreate.SFrame('/content/gdrive/My Drive/Turicreate/Classification/Week2/amazon_baby_subset.sframe/')"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TvS2bCICnrcM"},"source":["One column of this dataset is 'sentiment', corresponding to the class label with +1 indicating a review with positive sentiment and -1 indicating one with negative sentiment."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaocCatcnrcN","executionInfo":{"status":"ok","timestamp":1630101360826,"user_tz":420,"elapsed":187,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"ffe8b3bd-cd5b-44a4-a5e4-76440dcac0e7"},"source":["products['sentiment']"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype: int\n","Rows: 53072\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... ]"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"y6UAHcf2nrcO"},"source":["Let us quickly explore more of this dataset.  The 'name' column indicates the name of the product.  Here we list the first 10 products in the dataset.  We then count the number of positive and negative reviews."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04fOiEFOnrcP","executionInfo":{"status":"ok","timestamp":1630101363066,"user_tz":420,"elapsed":182,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"da69408b-d52a-4a23-af9d-67aea36a2de2"},"source":["products.head(10)['name']\n","#products"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype: str\n","Rows: 10\n","[\"Stop Pacifier Sucking without tears with Thumbuddy To Love's Binky Fairy Puppet and Adorable Book\", \"Nature's Lullabies Second Year Sticker Calendar\", \"Nature's Lullabies Second Year Sticker Calendar\", 'Lamaze Peekaboo, I Love You', \"SoftPlay Peek-A-Boo Where's Elmo A Children's Book\", 'Our Baby Girl Memory Book', 'Hunnt&reg; Falling Flowers and Birds Kids Nursery Home Decor Vinyl Mural Art Wall Paper Stickers', 'Blessed By Pope Benedict XVI Divine Mercy Full Color Medal', 'Cloth Diaper Pins Stainless Steel Traditional Safety Pin (Black)', 'Cloth Diaper Pins Stainless Steel Traditional Safety Pin (Black)']"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1PGImDpdnrcQ","executionInfo":{"status":"ok","timestamp":1630101365750,"user_tz":420,"elapsed":1126,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"2af0e6fe-bf21-48a9-dde6-24b1153c0453"},"source":["print('# of positive reviews =', len(products[products['sentiment']==1]))\n","print('# of negative reviews =', len(products[products['sentiment']==-1]))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["# of positive reviews = 26579\n","# of negative reviews = 26493\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZI9tcX2nrcR"},"source":["**Note:** For this assignment, we eliminated class imbalance by choosing \n","a subset of the data with a similar number of positive and negative reviews. \n","\n","## Apply text cleaning on the review data\n","\n","In this section, we will perform some simple feature cleaning using **SFrames**. The last assignment used all words in building bag-of-words features, but here we limit ourselves to 193 words (for simplicity). We compiled a list of 193 most frequent words into a JSON file. \n","\n","Now, we will load these words from this JSON file:"]},{"cell_type":"code","metadata":{"id":"fTEpKHjCnrcS","executionInfo":{"status":"ok","timestamp":1630101549490,"user_tz":420,"elapsed":412,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["import json\n","with open('/content/gdrive/My Drive/Turicreate/Classification/Week2/important_words.json', 'r') as f: # Reads the list of most frequent words\n","    important_words = json.load(f)\n","important_words = [str(s) for s in important_words]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjHst-FVnrcS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630101578227,"user_tz":420,"elapsed":161,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"163d7157-8aa9-4de8-cac2-9f90972b3d84"},"source":["print(important_words)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["['baby', 'one', 'great', 'love', 'use', 'would', 'like', 'easy', 'little', 'seat', 'old', 'well', 'get', 'also', 'really', 'son', 'time', 'bought', 'product', 'good', 'daughter', 'much', 'loves', 'stroller', 'put', 'months', 'car', 'still', 'back', 'used', 'recommend', 'first', 'even', 'perfect', 'nice', 'bag', 'two', 'using', 'got', 'fit', 'around', 'diaper', 'enough', 'month', 'price', 'go', 'could', 'soft', 'since', 'buy', 'room', 'works', 'made', 'child', 'keep', 'size', 'small', 'need', 'year', 'big', 'make', 'take', 'easily', 'think', 'crib', 'clean', 'way', 'quality', 'thing', 'better', 'without', 'set', 'new', 'every', 'cute', 'best', 'bottles', 'work', 'purchased', 'right', 'lot', 'side', 'happy', 'comfortable', 'toy', 'able', 'kids', 'bit', 'night', 'long', 'fits', 'see', 'us', 'another', 'play', 'day', 'money', 'monitor', 'tried', 'thought', 'never', 'item', 'hard', 'plastic', 'however', 'disappointed', 'reviews', 'something', 'going', 'pump', 'bottle', 'cup', 'waste', 'return', 'amazon', 'different', 'top', 'want', 'problem', 'know', 'water', 'try', 'received', 'sure', 'times', 'chair', 'find', 'hold', 'gate', 'open', 'bottom', 'away', 'actually', 'cheap', 'worked', 'getting', 'ordered', 'came', 'milk', 'bad', 'part', 'worth', 'found', 'cover', 'many', 'design', 'looking', 'weeks', 'say', 'wanted', 'look', 'place', 'purchase', 'looks', 'second', 'piece', 'box', 'pretty', 'trying', 'difficult', 'together', 'though', 'give', 'started', 'anything', 'last', 'company', 'come', 'returned', 'maybe', 'took', 'broke', 'makes', 'stay', 'instead', 'idea', 'head', 'said', 'less', 'went', 'working', 'high', 'unit', 'seems', 'picture', 'completely', 'wish', 'buying', 'babies', 'won', 'tub', 'almost', 'either']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YL7G1TpKnrcS"},"source":["Now, we will perform 2 simple data transformations:\n","\n","1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n","2. Compute word counts (only for **important_words**)\n","\n","We start with *Step 1* which can be done as follows:"]},{"cell_type":"code","metadata":{"id":"QSHnyizOnrcT","executionInfo":{"status":"ok","timestamp":1630101574720,"user_tz":420,"elapsed":3200,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["import string \n","def remove_punctuation(text):\n","    try: # python 2.x\n","        text = text.translate(None, string.punctuation) \n","    except: # python 3.x\n","        translator = text.maketrans('', '', string.punctuation)\n","        text = text.translate(translator)\n","    return text\n","\n","products['review_clean'] = products['review'].apply(remove_punctuation)"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5_6shvWonrcT"},"source":["Now we proceed with *Step 2*. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text.\n","\n","\n","**Note:** There are several ways of doing this. In this assignment, we use the built-in *count* function for Python lists. Each review string is first split into individual words and the number of occurances of a given word is counted."]},{"cell_type":"code","metadata":{"id":"ZEn2IC41nrcT","executionInfo":{"status":"ok","timestamp":1630101715032,"user_tz":420,"elapsed":1794,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["for word in important_words:\n","    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Y6c00qRnrcU"},"source":["The SFrame **products** now contains one column for each of the 193 **important_words**. As an example, the column **perfect** contains a count of the number of times the word **perfect** occurs in each of the reviews."]},{"cell_type":"code","metadata":{"id":"mia7Uh6TnrcU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630101717789,"user_tz":420,"elapsed":227,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"2d9e64b1-aa97-4454-d251-51bc71f56c67"},"source":["products['perfect']"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype: int\n","Rows: 53072\n","[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"2fAu0j25nrcU"},"source":["Now, write some code to compute the number of product reviews that contain the word **perfect**.\n","\n","**Hint**: \n","* First create a column called `contains_perfect` which is set to 1 if the count of the word **perfect** (stored in column **perfect**) is >= 1.\n","* Sum the number of 1s in the column `contains_perfect`."]},{"cell_type":"code","metadata":{"id":"kWPudorznrcU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630107116556,"user_tz":420,"elapsed":151,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"1328e3be-3695-4a8d-f7ff-72c73bc277d4"},"source":["#def contains_word_perfect(s):\n","#  if (s >= 1):\n","#    return 1\n","#  else:\n","#    return 0\n","#products['contains_perfect'] = products['perfect'].apply(lambda s: contains_word_perfect(s))\n","products['contains_perfect'] = products['perfect'] >= 1\n","num_perfect = sum(products['contains_perfect'])\n","#num_perfect = (products['perfect'] >= 1).sum()\n","print (num_perfect)"],"execution_count":82,"outputs":[{"output_type":"stream","text":["2955\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wnx1LU4BnrcU"},"source":["**Quiz Question**. How many reviews contain the word **perfect**?"]},{"cell_type":"markdown","metadata":{"id":"xQ1i9GYSnrcV"},"source":["## Convert SFrame to NumPy array\n","\n","As you have seen previously, NumPy is a powerful library for doing matrix manipulation. Let us convert our data to matrices and then implement our algorithms with matrices.\n","\n","First, make sure you can perform the following import. If it doesn't work, you need to go back to the terminal and run\n","\n","`pip install numpy`."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"oGNid4u6nrcV","executionInfo":{"status":"ok","timestamp":1630101833091,"user_tz":420,"elapsed":3,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["import numpy as np"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFjm3nkwnrcV"},"source":["We now provide you with a function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. Note that the feature matrix includes an additional column 'intercept' to take account of the intercept term."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"G0D-4zVanrcW","executionInfo":{"status":"ok","timestamp":1630101834740,"user_tz":420,"elapsed":172,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["def get_numpy_data(data_sframe, features, label):\n","    data_sframe['intercept'] = 1\n","    features = ['intercept'] + features\n","    features_sframe = data_sframe[features]\n","    feature_matrix = features_sframe.to_numpy()\n","    label_sarray = data_sframe[label]\n","    label_array = label_sarray.to_numpy()\n","    return(feature_matrix, label_array)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dB3zLmq0nrcW"},"source":["Let us convert the data into NumPy arrays."]},{"cell_type":"code","metadata":{"id":"UuY-xuVgnrcW","executionInfo":{"status":"ok","timestamp":1630102104195,"user_tz":420,"elapsed":268444,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["# Warning: This may take a few minutes...\n","feature_matrix, sentiment = get_numpy_data(products, important_words, 'sentiment') "],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YoLvCt9VnrcW"},"source":["**Are you running this notebook on an Amazon EC2 t2.micro instance?** (If you are using your own machine, please skip this section)\n","\n","It has been reported that t2.micro instances do not provide sufficient power to complete the conversion in acceptable amount of time. For interest of time, please refrain from running `get_numpy_data` function. Instead, download the [binary file](https://s3.amazonaws.com/static.dato.com/files/coursera/course-3/numpy-arrays/module-3-assignment-numpy-arrays.npz) containing the four NumPy arrays you'll need for the assignment. To load the arrays, run the following commands:\n","```\n","arrays = np.load('module-3-assignment-numpy-arrays.npz')\n","feature_matrix, sentiment = arrays['feature_matrix'], arrays['sentiment']\n","```"]},{"cell_type":"code","metadata":{"id":"HV3EyQ3CnrcW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630102112753,"user_tz":420,"elapsed":159,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"871fbfeb-d106-4f3b-c2f8-d81bc6dd4f45"},"source":["feature_matrix.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(53072, 194)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"A1qek9ZLnrcX"},"source":["**Quiz Question:** How many features are there in the **feature_matrix**?\n","\n","**Quiz Question:** Assuming that the intercept is present, how does the number of features in **feature_matrix** relate to the number of features in the logistic regression model?"]},{"cell_type":"markdown","metadata":{"id":"aCGgX_8qnrcX"},"source":["Now, let us see what the **sentiment** column looks like:"]},{"cell_type":"code","metadata":{"id":"LKJiKWO3nrcX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630102637385,"user_tz":420,"elapsed":152,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"34dca0e6-4fb2-44eb-c6dd-676ab03fb8ba"},"source":["sentiment"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 1,  1,  1, ..., -1, -1, -1])"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"HtLarDB2nrcX"},"source":["## Estimating conditional probability with link function"]},{"cell_type":"markdown","metadata":{"id":"UmDxuEQgnrcX"},"source":["Recall from lecture that the link function is given by:\n","$$\n","P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n","$$\n","\n","where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review  $\\mathbf{x}_i$. Complete the following function that implements the link function:"]},{"cell_type":"code","metadata":{"id":"sXYHzQEOnrcX","executionInfo":{"status":"ok","timestamp":1630103255735,"user_tz":420,"elapsed":157,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["'''\n","produces probablistic estimate for P(y_i = +1 | x_i, w).\n","estimate ranges between 0 and 1.\n","'''\n","def predict_probability(feature_matrix, coefficients):\n","    # Take dot product of feature_matrix and coefficients  \n","    # YOUR CODE HERE\n","    dot_product = np.dot(feature_matrix, coefficients)\n","    \n","    # Compute P(y_i = +1 | x_i, w) using the link function\n","    # YOUR CODE HERE\n","    predictions = 1/(1 + np.exp(-dot_product))\n","    \n","    # return predictions\n","    return predictions"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DeR3enHhnrcX"},"source":["**Aside**. How the link function works with matrix algebra\n","\n","Since the word counts are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n","$$\n","[\\text{feature_matrix}] =\n","\\left[\n","\\begin{array}{c}\n","h(\\mathbf{x}_1)^T \\\\\n","h(\\mathbf{x}_2)^T \\\\\n","\\vdots \\\\\n","h(\\mathbf{x}_N)^T\n","\\end{array}\n","\\right] =\n","\\left[\n","\\begin{array}{cccc}\n","h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n","h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n","h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n","\\end{array}\n","\\right]\n","$$\n","\n","By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n","$$\n","[\\text{score}] =\n","[\\text{feature_matrix}]\\mathbf{w} =\n","\\left[\n","\\begin{array}{c}\n","h(\\mathbf{x}_1)^T \\\\\n","h(\\mathbf{x}_2)^T \\\\\n","\\vdots \\\\\n","h(\\mathbf{x}_N)^T\n","\\end{array}\n","\\right]\n","\\mathbf{w}\n","= \\left[\n","\\begin{array}{c}\n","h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n","h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n","\\vdots \\\\\n","h(\\mathbf{x}_N)^T\\mathbf{w}\n","\\end{array}\n","\\right]\n","= \\left[\n","\\begin{array}{c}\n","\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n","\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n","\\vdots \\\\\n","\\mathbf{w}^T h(\\mathbf{x}_N)\n","\\end{array}\n","\\right]\n","$$"]},{"cell_type":"markdown","metadata":{"id":"EWsGk9zPnrcY"},"source":["**Checkpoint**\n","\n","Just to make sure you are on the right track, we have provided a few examples. If your `predict_probability` function is implemented correctly, then the outputs will match:"]},{"cell_type":"code","metadata":{"id":"d5IOsqn0nrcY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630103268629,"user_tz":420,"elapsed":151,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"715506d5-ab91-424e-eaed-61162d4bc597"},"source":["dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n","dummy_coefficients = np.array([1., 3., -1.])\n","\n","correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),          1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n","correct_predictions = np.array( [ 1./(1+np.exp(-correct_scores[0])), 1./(1+np.exp(-correct_scores[1])) ] )\n","\n","print('The following outputs must match ')\n","print('------------------------------------------------')\n","print('correct_predictions           =', correct_predictions)\n","print('output of predict_probability =', predict_probability(dummy_feature_matrix, dummy_coefficients))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["The following outputs must match \n","------------------------------------------------\n","correct_predictions           = [0.98201379 0.26894142]\n","output of predict_probability = [0.98201379 0.26894142]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KcYDxpvLnrcY"},"source":["## Compute derivative of log likelihood with respect to a single coefficient\n","\n","Recall from lecture:\n","$$\n","\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n","$$\n","\n","We will now write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n","* `errors` vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n","* `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$. \n","\n","Complete the following code block:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"z52uoNAWnrcY","executionInfo":{"status":"ok","timestamp":1630109076764,"user_tz":420,"elapsed":149,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["def feature_derivative(errors, feature):     \n","    # Compute the dot product of errors and feature\n","    derivative = np.dot(errors, feature)\n","    \n","    # Return the derivative\n","    return derivative"],"execution_count":98,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YhyjqmKXnrcY"},"source":["In the main lecture, our focus was on the likelihood.  In the advanced optional video, however, we introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n","\n","The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n","\n","$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n","\n","We provide a function to compute the log likelihood for the entire dataset. "]},{"cell_type":"code","metadata":{"id":"xU-JQx-snrcZ","executionInfo":{"status":"ok","timestamp":1630103701888,"user_tz":420,"elapsed":149,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n","    indicator = (sentiment==+1)\n","    scores = np.dot(feature_matrix, coefficients)\n","    logexp = np.log(1. + np.exp(-scores))\n","    \n","    # Simple check to prevent overflow\n","    mask = np.isinf(logexp)\n","    logexp[mask] = -scores[mask]\n","    \n","    lp = np.sum((indicator-1)*scores - logexp)\n","    return lp"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXu0o7i2nrcZ"},"source":["**Checkpoint**\n","\n","Just to make sure we are on the same page, run the following code block and check that the outputs match."]},{"cell_type":"code","metadata":{"id":"7tLaQ3pJnrcZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630103693423,"user_tz":420,"elapsed":154,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"e3e50e1c-53eb-4f4a-b174-475601286e03"},"source":["dummy_feature_matrix = np.array([[1.,2.,3.], [1.,-1.,-1]])\n","dummy_coefficients = np.array([1., 3., -1.])\n","dummy_sentiment = np.array([-1, 1])\n","\n","correct_indicators  = np.array( [ -1==+1,                                       1==+1 ] )\n","correct_scores      = np.array( [ 1.*1. + 2.*3. + 3.*(-1.),                     1.*1. + (-1.)*3. + (-1.)*(-1.) ] )\n","correct_first_term  = np.array( [ (correct_indicators[0]-1)*correct_scores[0],  (correct_indicators[1]-1)*correct_scores[1] ] )\n","correct_second_term = np.array( [ np.log(1. + np.exp(-correct_scores[0])),      np.log(1. + np.exp(-correct_scores[1])) ] )\n","\n","correct_ll          =      sum( [ correct_first_term[0]-correct_second_term[0], correct_first_term[1]-correct_second_term[1] ] ) \n","\n","print('The following outputs must match ')\n","print('------------------------------------------------')\n","print('correct_log_likelihood           =', correct_ll)\n","print('output of compute_log_likelihood =', compute_log_likelihood(dummy_feature_matrix, dummy_sentiment, dummy_coefficients))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["The following outputs must match \n","------------------------------------------------\n","correct_log_likelihood           = -5.331411615436032\n","output of compute_log_likelihood = -5.331411615436032\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e3Mj8ej3nrcZ"},"source":["## Taking gradient steps"]},{"cell_type":"markdown","metadata":{"id":"oMyABqienrcZ"},"source":["Now we are ready to implement our own logistic regression. All we have to do is to write a gradient ascent function that takes gradient steps towards the optimum. \n","\n","Complete the following function to solve the logistic regression model using gradient ascent:"]},{"cell_type":"code","metadata":{"id":"CU6nP8pvnrcZ","executionInfo":{"status":"ok","timestamp":1630109080792,"user_tz":420,"elapsed":160,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["from math import sqrt\n","\n","def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n","    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n","    for itr in range(max_iter):\n","\n","        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n","        # YOUR CODE HERE\n","        predictions = predict_probability(feature_matrix, coefficients)\n","        \n","        # Compute indicator value for (y_i = +1)\n","        indicator = (sentiment==+1)\n","        \n","        # Compute the errors as indicator - predictions\n","        errors = indicator - predictions\n","        for j in range(len(coefficients)): # loop over each coefficient\n","            \n","            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n","            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n","            # YOUR CODE HERE\n","            derivative = feature_derivative(errors, feature_matrix[:,j])\n","            \n","            # add the step size times the derivative to the current coefficient\n","            ## YOUR CODE HERE\n","            coefficients[j] += step_size * derivative\n","        \n","        # Checking whether log likelihood is increasing\n","        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n","        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n","            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n","            print('iteration %*d: log likelihood of observed labels = %.8f' % \\\n","                (int(np.ceil(np.log10(max_iter))), itr, lp))\n","    return coefficients"],"execution_count":99,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3fq8fi4nrca"},"source":["Now, let us run the logistic regression solver."]},{"cell_type":"code","metadata":{"id":"ShtkygGenrca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630109107265,"user_tz":420,"elapsed":23924,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"5545a43a-7fbc-4470-a039-46054bb4b967"},"source":["coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients=np.zeros(194),\n","                                   step_size=1e-7, max_iter=301)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["iteration   0: log likelihood of observed labels = -36780.91768478\n","iteration   1: log likelihood of observed labels = -36775.13434712\n","iteration   2: log likelihood of observed labels = -36769.35713564\n","iteration   3: log likelihood of observed labels = -36763.58603240\n","iteration   4: log likelihood of observed labels = -36757.82101962\n","iteration   5: log likelihood of observed labels = -36752.06207964\n","iteration   6: log likelihood of observed labels = -36746.30919497\n","iteration   7: log likelihood of observed labels = -36740.56234821\n","iteration   8: log likelihood of observed labels = -36734.82152213\n","iteration   9: log likelihood of observed labels = -36729.08669961\n","iteration  10: log likelihood of observed labels = -36723.35786366\n","iteration  11: log likelihood of observed labels = -36717.63499744\n","iteration  12: log likelihood of observed labels = -36711.91808422\n","iteration  13: log likelihood of observed labels = -36706.20710739\n","iteration  14: log likelihood of observed labels = -36700.50205049\n","iteration  15: log likelihood of observed labels = -36694.80289716\n","iteration  20: log likelihood of observed labels = -36666.39512033\n","iteration  30: log likelihood of observed labels = -36610.01327118\n","iteration  40: log likelihood of observed labels = -36554.19728365\n","iteration  50: log likelihood of observed labels = -36498.93316099\n","iteration  60: log likelihood of observed labels = -36444.20783914\n","iteration  70: log likelihood of observed labels = -36390.00909449\n","iteration  80: log likelihood of observed labels = -36336.32546144\n","iteration  90: log likelihood of observed labels = -36283.14615871\n","iteration 100: log likelihood of observed labels = -36230.46102347\n","iteration 200: log likelihood of observed labels = -35728.89418769\n","iteration 300: log likelihood of observed labels = -35268.51212683\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_qMdQinqnrca"},"source":["**Quiz Question:** As each iteration of gradient ascent passes, does the log likelihood increase or decrease?"]},{"cell_type":"markdown","metadata":{"id":"IO_whZGrnrca"},"source":["## Predicting sentiments"]},{"cell_type":"markdown","metadata":{"id":"DLSqnEzenrca"},"source":["Recall from lecture that class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n","$$\n","\\hat{y}_i = \n","\\left\\{\n","\\begin{array}{ll}\n","      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n","      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n","\\end{array} \n","\\right.\n","$$\n","\n","Now, we will write some code to compute class predictions. We will do this in two steps:\n","* **Step 1**: First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n","* **Step 2**: Using the formula above, compute the class predictions from the scores.\n","\n","Step 1 can be implemented as follows:"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Gs1kbDEQnrca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630108777641,"user_tz":420,"elapsed":218,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"c00020cc-1258-4c2b-bd19-b347cd6c4013"},"source":["# Compute the scores as a dot product between feature_matrix and coefficients.\n","scores = np.dot(feature_matrix, coefficients)\n","print (coefficients)"],"execution_count":97,"outputs":[{"output_type":"stream","text":["[ 5.16220157e-03  1.55656966e-02 -8.50204675e-03  6.65460842e-02\n","  6.58907629e-02  5.01743882e-03 -5.38601484e-02 -3.50488413e-03\n","  6.47945868e-02  4.54356263e-02  3.98353364e-03  2.00775410e-02\n","  3.01350011e-02 -2.87115530e-02  1.52161964e-02  2.72592062e-04\n","  1.19448177e-02 -1.82461935e-02 -1.21706420e-02 -4.15110334e-02\n","  2.76820391e-03  1.77031999e-02 -4.39700067e-03  4.49764014e-02\n","  9.90916464e-03  8.99239081e-04 -1.36219516e-03  1.26859357e-02\n","  8.26466695e-03 -2.77426972e-02  6.10128809e-04  1.54084501e-02\n"," -1.32134753e-02 -3.00512492e-02  2.97399371e-02  1.84087080e-02\n","  2.86178752e-03 -1.05768015e-02 -6.57350362e-04 -1.01476555e-02\n"," -4.79579528e-03  7.50891810e-03  4.27938289e-03  3.06785501e-03\n"," -2.20317661e-03  9.57273354e-03  9.91666827e-05 -1.98462567e-02\n","  1.75702722e-02  1.55478612e-03 -1.77375440e-02  9.78324102e-03\n","  1.17031606e-02 -7.35345937e-03 -6.08714030e-03  6.43766808e-03\n","  1.07159665e-02 -3.05345476e-03  7.17190727e-03  5.73320003e-03\n","  4.60661876e-03 -5.20588421e-03  6.71012331e-03  9.03281814e-03\n","  1.74563147e-03  6.00279979e-03  1.20181744e-02 -1.83594607e-02\n"," -6.91010811e-03 -1.38687273e-02 -1.50406590e-02  5.92353611e-03\n","  5.67478991e-03 -5.28786220e-03  3.08147864e-03  5.53751236e-03\n","  1.49917916e-02 -3.35666000e-04 -3.30695153e-02 -4.78990943e-03\n"," -6.41368859e-03  7.99938935e-03 -8.61390444e-04  1.68052959e-02\n","  1.32539901e-02  1.72307051e-03  2.98030675e-03  8.58284300e-03\n","  1.17082481e-02  2.80825907e-03  2.18724016e-03  1.68824711e-02\n"," -4.65973741e-03  1.51368285e-03 -1.09509122e-02  9.17842898e-03\n"," -1.88572281e-04 -3.89820373e-02 -2.44821005e-02 -1.87023714e-02\n"," -2.13943485e-02 -1.29690465e-02 -1.71378670e-02 -1.37566767e-02\n"," -1.49770449e-02 -5.10287978e-03 -2.89789761e-02 -1.48663194e-02\n"," -1.28088380e-02 -1.07709355e-02 -6.95286915e-03 -5.04082164e-03\n"," -9.25914404e-03 -2.40427481e-02 -2.65927785e-02 -1.97320937e-03\n"," -5.04127508e-03 -7.00791912e-03 -3.48088523e-03 -6.40958916e-03\n"," -4.07497010e-03 -6.30054296e-03 -1.09187932e-02 -1.26051900e-02\n"," -1.66895314e-03 -7.76418781e-03 -5.15960485e-04 -1.94199551e-03\n"," -1.24761586e-03 -5.01291731e-03 -9.12049191e-03 -7.22098801e-03\n"," -8.31782981e-03 -5.60573348e-03 -1.47098335e-02 -9.31520819e-03\n"," -2.22034402e-03 -7.07573098e-03 -5.10115608e-03 -8.93572862e-03\n"," -1.27545713e-02 -7.04171991e-03 -9.76219676e-04  4.12091713e-04\n","  8.29251160e-04  2.64661064e-03 -7.73228782e-03  1.53471164e-03\n"," -7.37263060e-03 -3.73694386e-03 -3.81416409e-03 -1.64575145e-03\n"," -3.31887732e-03  1.22257832e-03  1.36699286e-05 -3.01866601e-03\n"," -1.02826343e-02 -1.06691327e-02  2.23639046e-03 -9.87424798e-03\n"," -1.02192048e-02 -3.41330929e-03  3.34489960e-03 -3.50984516e-03\n"," -6.26283150e-03 -7.22419943e-03 -5.47016154e-03 -1.25063947e-02\n"," -2.47805699e-03 -1.60017985e-02 -6.40098934e-03 -4.26644386e-03\n"," -1.55376990e-02  2.31349237e-03 -9.06653337e-03 -6.30012672e-03\n"," -1.21010303e-02 -3.02578875e-03 -6.76289718e-03 -5.65498722e-03\n"," -6.87050239e-03 -1.18950595e-02 -1.86489236e-04 -1.15230476e-02\n","  2.81533219e-03 -8.10150295e-03 -1.00062131e-02  4.02037651e-03\n"," -5.44300346e-03  2.85818985e-03  1.19885003e-04 -6.47587687e-03\n"," -1.14493516e-03 -7.09205934e-03]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7xu9NRZRnrcb"},"source":["Now, complete the following code block for **Step 2** to compute the class predictions using the **scores** obtained above:"]},{"cell_type":"code","metadata":{"id":"xFXgKqEInrcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630108561910,"user_tz":420,"elapsed":168,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"800392b0-3035-46ce-cf6c-31f2aa2d4482"},"source":["def class_predictions(score):\n","  if (score > 0):\n","    return 1\n","  else:\n","    return -1\n","\n","#x = np.array([1,4,5,-7, 6, 5])\n","predict = np.vectorize(class_predictions)\n","#print (predict(x))\n","#num = (x == 5).sum()\n","#print (num)\n","predictions = predict(scores)\n","num_positive = (scores > 0).sum()\n","print (num_positive)"],"execution_count":90,"outputs":[{"output_type":"stream","text":["25126\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NxBDyB57nrcb"},"source":["**Quiz Question:** How many reviews were predicted to have positive sentiment?"]},{"cell_type":"code","metadata":{"id":"C7waQ170nrcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630108033512,"user_tz":420,"elapsed":153,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"33c67fd6-1738-4310-9ede-bdd6410dcc41"},"source":["x = [1,2,3]\n","y = [1,3,3]\n","z = (y == x)\n","print(z)"],"execution_count":87,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GsOnm9jtnrcb"},"source":["## Measuring accuracy\n","\n","We will now measure the classification accuracy of the model. Recall from the lecture that the classification accuracy can be computed as follows:\n","\n","$$\n","\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n","$$\n","\n","Complete the following code block to compute the accuracy of the model."]},{"cell_type":"code","metadata":{"id":"ya8_c2eTnrcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630108564887,"user_tz":420,"elapsed":174,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"eee9aa88-18af-4e6b-b9c3-e451ea0cf757"},"source":["#def num_wrong_predictions(d):\n","#  if (d == 0):\n","#    return 0\n","#  else:\n","#    return 1\n","mistakes = predictions - products['sentiment']\n","num_mistakes = (mistakes != 0).sum() # YOUR CODE HERE\n","#num_mistakes_func = np.vectorize(num_wrong_predictions)\n","#num_mistakes = num_mistakes_func(mistakes).sum()\n","accuracy = (len(products) - num_mistakes)/len(products) # YOUR CODE HERE\n","print(\"-----------------------------------------------------\")\n","print('# Reviews   correctly classified =', len(products) - num_mistakes)\n","print('# Reviews incorrectly classified =', num_mistakes)\n","print('# Reviews total                  =', len(products))\n","print(\"-----------------------------------------------------\")\n","print('Accuracy = %.2f' % accuracy)"],"execution_count":91,"outputs":[{"output_type":"stream","text":["-----------------------------------------------------\n","# Reviews   correctly classified = 39903\n","# Reviews incorrectly classified = 13169\n","# Reviews total                  = 53072\n","-----------------------------------------------------\n","Accuracy = 0.75\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w19NKcuLnrcb"},"source":["**Quiz Question**: What is the accuracy of the model on predictions made above? (round to 2 digits of accuracy)"]},{"cell_type":"markdown","metadata":{"id":"yjkTFQgSnrcb"},"source":["## Which words contribute most to positive & negative sentiments?"]},{"cell_type":"markdown","metadata":{"id":"Vcm-Tmblnrcc"},"source":["Recall that in Module 2 assignment, we were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n","* Treat each coefficient as a tuple, i.e. (**word**, **coefficient_value**).\n","* Sort all the (**word**, **coefficient_value**) tuples by **coefficient_value** in descending order."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TUDNFm5pnrcc","executionInfo":{"status":"ok","timestamp":1630109116423,"user_tz":420,"elapsed":150,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}}},"source":["coefficients = list(coefficients[1:]) # exclude intercept\n","word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n","word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"],"execution_count":101,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qawqauaznrcc"},"source":["Now, **word_coefficient_tuples** contains a sorted list of (**word**, **coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive."]},{"cell_type":"markdown","metadata":{"id":"N4h6YEgEnrcc"},"source":["### Ten \"most positive\" words\n","\n","Now, we compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."]},{"cell_type":"code","metadata":{"id":"-T7HqsPlnrcc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630109117958,"user_tz":420,"elapsed":153,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"ef483602-632b-4262-82b0-0b36b82dc66f"},"source":["word_coefficient_tuples[0:10]"],"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('great', 0.0665460841704577),\n"," ('love', 0.06589076292212327),\n"," ('easy', 0.0647945868025784),\n"," ('little', 0.04543562630842137),\n"," ('loves', 0.04497640139490604),\n"," ('well', 0.030135001092107084),\n"," ('perfect', 0.02973993710496846),\n"," ('old', 0.020077541034775385),\n"," ('nice', 0.018408707995268992),\n"," ('daughter', 0.017703199905701694)]"]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"n0BAggR0nrcc"},"source":["**Quiz Question:** Which word is **not** present in the top 10 \"most positive\" words?\n","\n","- love\n","- easy\n","- great\n","- perfect\n","- cheap"]},{"cell_type":"markdown","metadata":{"id":"Cf20PZyUnrcc"},"source":["### Ten \"most negative\" words\n","\n","Next, we repeat this exercise on the 10 most negative words.  That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."]},{"cell_type":"code","metadata":{"id":"hoEVRvE-nrcd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630109120612,"user_tz":420,"elapsed":160,"user":{"displayName":"Vighnesh Sridhar","photoUrl":"","userId":"07506833859532900314"}},"outputId":"abb3ccaf-6daf-438c-ea70-d5f48abf1bf7"},"source":["word_coefficient_tuples"],"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('great', 0.0665460841704577),\n"," ('love', 0.06589076292212327),\n"," ('easy', 0.0647945868025784),\n"," ('little', 0.04543562630842137),\n"," ('loves', 0.04497640139490604),\n"," ('well', 0.030135001092107084),\n"," ('perfect', 0.02973993710496846),\n"," ('old', 0.020077541034775385),\n"," ('nice', 0.018408707995268992),\n"," ('daughter', 0.017703199905701694),\n"," ('soft', 0.017570272245602887),\n"," ('fits', 0.01688247107140872),\n"," ('happy', 0.016805295889768077),\n"," ('baby', 0.015565696580423505),\n"," ('recommend', 0.015408450108008665),\n"," ('also', 0.015216196422918844),\n"," ('best', 0.014991791565630266),\n"," ('comfortable', 0.013253990081584897),\n"," ('car', 0.012685935745813379),\n"," ('clean', 0.012018174433365525),\n"," ('son', 0.011944817713693953),\n"," ('bit', 0.01170824809312326),\n"," ('works', 0.011703160621987422),\n"," ('size', 0.0107159665162703),\n"," ('stroller', 0.009909164635972733),\n"," ('room', 0.009783241021568063),\n"," ('price', 0.00957273354359018),\n"," ('play', 0.009178428983984311),\n"," ('easily', 0.009032818138954405),\n"," ('kids', 0.008582843004346526),\n"," ('still', 0.008264666945374461),\n"," ('lot', 0.007999389349248327),\n"," ('around', 0.007508918097314342),\n"," ('need', 0.007171907270027089),\n"," ('take', 0.0067101233146641725),\n"," ('keep', 0.006437668081870187),\n"," ('crib', 0.006002799788638706),\n"," ('without', 0.005923536113220277),\n"," ('year', 0.005733200028487891),\n"," ('set', 0.005674789908056569),\n"," ('cute', 0.005537512364059028),\n"," ('use', 0.005017438816565411),\n"," ('big', 0.004606618761970606),\n"," ('diaper', 0.004279382890354768),\n"," ('wish', 0.004020376508099751),\n"," ('seat', 0.0039835336394136115),\n"," ('though', 0.0033448995950862924),\n"," ('every', 0.0030814786398540416),\n"," ('enough', 0.0030678550129901094),\n"," ('able', 0.002980306750661261),\n"," ('bag', 0.00286178751610521),\n"," ('babies', 0.0028581898513427404),\n"," ('seems', 0.002815332188646533),\n"," ('night', 0.002808259066683148),\n"," ('good', 0.002768203906397682),\n"," ('many', 0.0026466106385121296),\n"," ('makes', 0.002313492374049021),\n"," ('pretty', 0.0022363904560238454),\n"," ('long', 0.002187240162161944),\n"," ('think', 0.0017456314732199943),\n"," ('toy', 0.001723070510231438),\n"," ('since', 0.0015547861236921198),\n"," ('looking', 0.0015347116392846498),\n"," ('us', 0.0015136828514065374),\n"," ('purchase', 0.0012225783235106367),\n"," ('put', 0.000899239081415468),\n"," ('cover', 0.0008292511599959568),\n"," ('used', 0.0006101288089752652),\n"," ('found', 0.0004120917130065564),\n"," ('really', 0.00027259206246776175),\n"," ('won', 0.00011988500346780872),\n"," ('go', 9.916668274814268e-05),\n"," ('looks', 1.3669928550231003e-05),\n"," ('high', -0.00018648923566776286),\n"," ('day', -0.00018857228050011147),\n"," ('bottles', -0.0003356660003498168),\n"," ('chair', -0.0005159604846262286),\n"," ('using', -0.0006573503620923428),\n"," ('side', -0.000861390443768689),\n"," ('worth', -0.0009762196756223378),\n"," ('almost', -0.0011449351626316184),\n"," ('hold', -0.0012476158571719635),\n"," ('months', -0.00136219515734874),\n"," ('look', -0.001645751447384549),\n"," ('sure', -0.0016689531424132918),\n"," ('find', -0.0019419955109540173),\n"," ('amazon', -0.0019732093718961645),\n"," ('month', -0.0022031766056230706),\n"," ('getting', -0.002220344023969567),\n"," ('come', -0.0024780569860886567),\n"," ('second', -0.003018666010667183),\n"," ('head', -0.003025788749392799),\n"," ('small', -0.003053454759330871),\n"," ('place', -0.0033188773175081994),\n"," ('together', -0.0034133092891886515),\n"," ('want', -0.003480885231408433),\n"," ('like', -0.003504884133335208),\n"," ('give', -0.003509845157568247),\n"," ('say', -0.0037369438587485193),\n"," ('wanted', -0.0038141640936843104),\n"," ('know', -0.004074970100351703),\n"," ('took', -0.004266443864014709),\n"," ('much', -0.004397000674509432),\n"," ('see', -0.004659737406888564),\n"," ('purchased', -0.0047899094286734025),\n"," ('fit', -0.004795795278093019),\n"," ('gate', -0.005012917306197127),\n"," ('bottle', -0.005040821636249148),\n"," ('different', -0.005041275079058816),\n"," ('came', -0.005101156084295061),\n"," ('however', -0.005102879784877111),\n"," ('make', -0.005205884210607157),\n"," ('new', -0.005287862195972149),\n"," ('buying', -0.0054430034643825976),\n"," ('last', -0.005470161541392434),\n"," ('actually', -0.005605733475251416),\n"," ('less', -0.005654987222257695),\n"," ('child', -0.006087140302347554),\n"," ('started', -0.006262831503162304),\n"," ('instead', -0.006300126718179462),\n"," ('water', -0.006300542962225543),\n"," ('maybe', -0.006400989344627022),\n"," ('problem', -0.006409589155003782),\n"," ('right', -0.006413688585050476),\n"," ('tub', -0.006475876873018087),\n"," ('said', -0.006762897175112026),\n"," ('went', -0.006870502394402382),\n"," ('quality', -0.006910108112146036),\n"," ('pump', -0.006952869149061944),\n"," ('top', -0.0070079191164656805),\n"," ('part', -0.007041719908427805),\n"," ('ordered', -0.0070757309816473464),\n"," ('either', -0.007092059344135139),\n"," ('bottom', -0.007220988012088567),\n"," ('anything', -0.007224199425910096),\n"," ('made', -0.0073534593690945255),\n"," ('weeks', -0.007372630597764411),\n"," ('design', -0.007732287817370991),\n"," ('times', -0.007764187808314245),\n"," ('picture', -0.008101502952676016),\n"," ('away', -0.00831782980769609),\n"," ('one', -0.00850204674575737),\n"," ('milk', -0.008935728615993479),\n"," ('stay', -0.009066533373590216),\n"," ('open', -0.009120491910748275),\n"," ('cup', -0.009259144043189976),\n"," ('worked', -0.009315208191956718),\n"," ('trying', -0.009874247980135874),\n"," ('completely', -0.010006213059178752),\n"," ('got', -0.01014765551165753),\n"," ('difficult', -0.010219204790778673),\n"," ('piece', -0.010282634317735408),\n"," ('two', -0.01057680153779288),\n"," ('box', -0.010669132724912933),\n"," ('going', -0.010770935491004765),\n"," ('try', -0.010918793208792996),\n"," ('another', -0.010950912169649833),\n"," ('unit', -0.011523047555497197),\n"," ('working', -0.011895059489782877),\n"," ('idea', -0.012101030310887904),\n"," ('bought', -0.012170641997242547),\n"," ('company', -0.012506394670958822),\n"," ('received', -0.012605189953662306),\n"," ('bad', -0.012754571329579797),\n"," ('something', -0.01280883802475081),\n"," ('never', -0.012969046546319293),\n"," ('first', -0.013213475301677047),\n"," ('hard', -0.013756676731261401),\n"," ('thing', -0.013868727265297582),\n"," ('cheap', -0.014709833465080665),\n"," ('reviews', -0.014866319449976984),\n"," ('plastic', -0.014977044903587944),\n"," ('better', -0.015040658977043393),\n"," ('broke', -0.01553769895565389),\n"," ('returned', -0.016001798500102516),\n"," ('item', -0.017137867010854787),\n"," ('buy', -0.017737543997218046),\n"," ('time', -0.01824619348608703),\n"," ('way', -0.018359460662945686),\n"," ('tried', -0.018702371424325834),\n"," ('could', -0.01984625666077721),\n"," ('thought', -0.021394348543682485),\n"," ('waste', -0.024042748071154956),\n"," ('monitor', -0.024482100545891717),\n"," ('return', -0.026592778462247283),\n"," ('back', -0.027742697230661334),\n"," ('get', -0.02871155298019258),\n"," ('disappointed', -0.028978976142317068),\n"," ('even', -0.030051249236035804),\n"," ('work', -0.03306951529475273),\n"," ('money', -0.038982037286487116),\n"," ('product', -0.041511033392108904),\n"," ('would', -0.05386014844520314)]"]},"metadata":{},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"3hmxftPonrcd"},"source":["**Quiz Question:** Which word is **not** present in the top 10 \"most negative\" words?\n","\n","- need\n","- work\n","- disappointed\n","- even\n","- return"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"bJSt8okInrcd"},"source":[""],"execution_count":null,"outputs":[]}]}